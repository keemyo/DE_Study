{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b2e58e1",
   "metadata": {},
   "source": [
    "# 01. Reduction Operations\n",
    "### 01-1. Reduction\n",
    "- 요소들을 모아서 하나로 합치는 작업\n",
    "- 많은 Spark의 연산들이 reduction에 해당됨\n",
    "\n",
    "# 02. 우리가 배운 것\n",
    "- Transformations & Actions \n",
    "- 지연 실행으로 인한 성능 최적화\n",
    "- Cache() & Persist()\n",
    "- Cluster Topology\n",
    "\n",
    "# 03. Transformations \n",
    "### 03-1. Parallel Transformations \n",
    "- 주로 변형을 적용시키는 작업들\n",
    "- map, flatmap, filter\n",
    "Action은 어떻게 분산된 환경에서 작동할까?\n",
    "\n",
    "# 04. Reduction이란?\n",
    "### 04-1. 대부분의 Action은 Reduction\n",
    "Reduction: 근접하는 요소들을 모아서 하나의 결과로 만드는 일\n",
    "- 파일 저장, collect()등과 같이 Reduction이 아닌 액션도 있다.\n",
    "\n",
    "# 05. 병렬처리 하기 가능한 예\n",
    "### 05-1. Parallel Reduction \n",
    "- 병렬 처리가 가능한 경우\n",
    "    - 두개의 요소들을 모아서 하나의 값으로 모으는 경우 각각이 독립적이어야함\n",
    "- 병렬 처리가 힘든 경우(병렬처리 하는 의미가 없는 경우)\n",
    "    - 순서대로가 아니고 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad55f2f",
   "metadata": {},
   "source": [
    "# 06. Reduction Actions\n",
    "- Reduce\n",
    "- Fold\n",
    "- GroupBy\n",
    "- Aggregate \n",
    "    \n",
    "### 06-1. Reduce\n",
    "- ***RDD.reduce(fuction)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abbed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 22:41:26 WARN Utils: Your hostname, Keemyoui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.35.79 instead (on interface en0)\n",
      "23/03/05 22:41:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 22:41:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/05 22:41:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/03/05 22:41:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('category-review-average')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.parallelize([1,2,3,4,5]).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f38983",
   "metadata": {},
   "source": [
    "##### 06-1-1. Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284a61ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4]).reduce(lambda x,y : (x*2)+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f4eca7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1,2,3,4) -> ((1*2+2)*2+3)*2+4 = 26\n",
    "sc.parallelize([1,2,3,4],1).reduce(lambda x,y : (x*2)+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e582473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1,2),(3,4) -> ((1*2+2)*2 + (3*2)+4) = 18\n",
    "sc.parallelize([1,2,3,4],2).reduce(lambda x,y : (x*2)+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8881a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4],3).reduce(lambda x,y : (x*2)+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0d0e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4],4).reduce(lambda x,y : (x*2)+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4328f44",
   "metadata": {},
   "source": [
    "##### 파티션이 어떻게 나뉠지 프로그래머가 정확하게 알기 어렵다\n",
    "- 연산의 순서와 상관 없이 결과값을 보장하려면\n",
    "    - 교환법칙(a*b = b*a)\n",
    "    - 결합법칙(a*b)*c = a*(b*c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4927d18",
   "metadata": {},
   "source": [
    "### 06-2. Fold\n",
    "- RDD.fold(zeroValue, Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e944a71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add \n",
    "sc.parallelize([1,2,3,4,5]).fold(0, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b35d3f",
   "metadata": {},
   "source": [
    "##### FOLD가 어떻게 되어있냐면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76947ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold(self, zeroValue, op):\n",
    "    \"\"\"\n",
    "    Aggregate the elements of each partition, and then the results \n",
    "    the partitions, using a given associative function and a neutral \"zero value.\"\n",
    "    \n",
    "    The function C{op(t1, t2)} is allowed to modify C{t1} and return it as its result value\n",
    "    to avoid object allocation; however, it shouldnot modify C{t2}.\n",
    "    \n",
    "    >>> from operator import add \n",
    "    >>> sc.parallelize ([1,2,3,4,5]).fold(0, add)\n",
    "    15\n",
    "    \"\"\"\n",
    "    \n",
    "    def func(iterator):\n",
    "        acc = zeroValue #Zerovalue를 acc 값에 담고\n",
    "        for obj in iterator:\n",
    "            acc = op(obj, acc) # acc에 op 연산을 누적\n",
    "        yield acc\n",
    "    vals = self.mapPartitions(func).collect() # 인자로 전달하는 함수를 파티션 단위로 전달하고 새로운 RDD로 변환\n",
    "    return reduce(op, vals, zeroValue) # return하기전 reduce를 적용, 파이썬 function 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f1120",
   "metadata": {},
   "source": [
    "##### 06-2-1. Fold & Partition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2563edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([2,3,4],4) # 파티션은 4개인데, 요소가 3개? -- 어느 파티션은 값이 없게 되겟죠?\n",
    "rdd.reduce(lambda x,y : x*y) # (2*3*4) = 24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a222eabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.fold(1, lambda x, y : x*y) # (1*2*3*4)= 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e95dfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#그러나\n",
    "rdd.reduce(lambda x, y : x+y) # (0+2+3+4) = 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ab0a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.fold(1, lambda x,y : x+y) \n",
    "# (1+1) + (1+2) + (1+3) + (1+4) = 14\n",
    "# 각 파티션의 시작값이 1이기 때문에 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e469604",
   "metadata": {},
   "source": [
    "### 06-2. GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2557a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.groupBy(<기준함수>)\n",
    "rdd = sc.parallelize([1,1,2,3,5,8])\n",
    "result = rdd.groupBy(lambda x: x%2).collect() # x를 2로 나눴을 때 나머지를 기준으로 그룹바이 \n",
    "sorted([(x, sorted(y)) for (x, y) in result]) # x: 나머지 , y는 원래 element를 sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43393b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [3, 6, 9]), (1, [1, 4, 7, 10]), (2, [2, 5])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 연습해보자\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,9,10])\n",
    "result = rdd.groupBy(lambda x: x%3).collect() \n",
    "sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d01f8",
   "metadata": {},
   "source": [
    "### 06-3. Aggregate\n",
    "- RDD 데이터 타입과 Action 결과 타입이 다를 경우 사용\n",
    "- 파티션 단위의 연산 결과를 합치는 과정을 거친다.\n",
    "- RDD.aggregate(zeroValue, seqOp, combOp) \n",
    "    - zeroValue: 각 파티션에서 누적할 시작 값 \n",
    "    - seqOp: 타입 변경 함수 (=map?) \n",
    "    - combOp: 합치는 함수 (=reduce)\n",
    "    \n",
    "- 많이 쓰이는 reduction action \n",
    "- 대부분의 데이터 작업은 \"크고 복잡한 데이터 타입\" -> \"정제된 데이터\" 로 바꾸는 작업\n",
    "- 배운것 외에도 여러가지 Operation이 존재\n",
    "- Key-Value RDD\n",
    "    - groupByKey\n",
    "    - reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ab6bb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] +1)) \n",
    "combOp = (lambda x, y: (x[0]+ y[0], x[1] + y[1])) # 파티션마다 값을 모아서 하나로 만들어쥐\n",
    "sc.parallelize([1,2,3,4]).aggregate((0,0), seqOp, combOp) #PairRdd(0,0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2affb6ff",
   "metadata": {},
   "source": [
    "- x[0] = 0, x[1] = 0 (zeroValue) \n",
    "    - x[0] + y = (0+1), x[1]+1 = (0+1) -> (1,1) \n",
    "    - x[0] + y = (1+2), x[1]+1 = (1+1) -> (3,2)\n",
    "\n",
    "- x[0] = 0, x[1] = 0 (zeroValue)\n",
    "    - x[0] + y = (0,3), x[1]+1 = (0+1) -> (3,1)\n",
    "    - x[0] + y = (3,4), x[1]+1 = (1+1) -> (7,2)\n",
    "    \n",
    "- x[0]=3, y[0] = 7, x[1] = 2, y[1] =2 \n",
    "    - x[0]+y[0]  x[1]+y[1] = 10,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af9644f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/06 16:13:40 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1214802 ms exceeds timeout 120000 ms\n",
      "23/03/06 16:13:40 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "sc.parallelize([]).aggregate((0,0) , seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118aa612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
