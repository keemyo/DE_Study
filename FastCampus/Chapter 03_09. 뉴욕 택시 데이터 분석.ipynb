{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2bdbd1",
   "metadata": {},
   "source": [
    "# 0. 재료준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99136979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 1. parquet파일을 csv로 바꿔보자\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "file_path = \"/Users/keemyohaan/Desktop/001.Python/004. Study/009.FastCampus/003.DE/01-spark/data/trips\"\n",
    "file_lst = [\n",
    "    \"yellow_tripdata_2021-01.parquet\"\n",
    "    , \"yellow_tripdata_2021-02.parquet\"\n",
    "    , \"yellow_tripdata_2021-03.parquet\"\n",
    "    , \"yellow_tripdata_2021-04.parquet\"\n",
    "    , \"yellow_tripdata_2021-05.parquet\"\n",
    "    , \"yellow_tripdata_2021-06.parquet\"\n",
    "    , \"yellow_tripdata_2021-07.parquet\"\n",
    "]\n",
    "\n",
    "# Parquet 파일 열기\n",
    "for i in range(len(file_lst)):\n",
    "    parquet_file = pq.read_table(f'{file_path}/{file_lst[i]}')\n",
    "\n",
    "    # Parquet 파일을 pandas DataFrame으로 변환\n",
    "    df = parquet_file.to_pandas()\n",
    "\n",
    "    # DataFrame을 CSV 파일로 저장\n",
    "    df.to_csv(f'{file_path}/{file_lst[i].split(\".parquet\")[0]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55e15e",
   "metadata": {},
   "source": [
    "# 1. 가보자고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46ec1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('taxi-analysis').getOrCreate()\n",
    "# 여러개 파일을 바로 가져온다.\n",
    "trip_files = \"/Users/keemyohaan/Desktop/001.Python/004. Study/009.FastCampus/003.DE/01-spark/data/trips/*\"\n",
    "zone_file = \"/Users/keemyohaan/Desktop/001.Python/004. Study/009.FastCampus/003.DE/01-spark/data/taxi+_zone_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c2ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df = spark.read.csv(f\"file:///{trip_files}\", inferSchema=True, header=True)\n",
    "zone_df = spark.read.csv(f\"file:///{zone_file}\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddadbbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()\n",
    "zone_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52869373",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView('trips')\n",
    "zone_df.createOrReplaceTempView('zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "197cc215",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    t.VendorID as vendor_id\n",
    "    , TO_DATE(t.tpep_pickup_datetime) as pickup_date\n",
    "    , TO_DATE(t.tpep_dropoff_datetime) as dropoff_date    \n",
    "    \n",
    "    , HOUR(t.tpep_pickup_datetime) as pickup_time\n",
    "    , HOUR(t.tpep_dropoff_datetime) as dropoff_time     \n",
    "\n",
    "    , t.passenger_count\n",
    "    , t.trip_distance\n",
    "    , t.fare_amount\n",
    "    , t.tolls_amount\n",
    "    , t.total_amount\n",
    "    , t.payment_type\n",
    "    , pz.Zone as pickup_zone\n",
    "    , dz.Zone as dropoff_zone\n",
    "FROM trips t\n",
    "LEFT JOIN zone pz\n",
    "    ON t.PULocationID = pz.LocationID\n",
    "LEFT JOIN \n",
    "    zone dz\n",
    "ON t.DOLocationID = dz.LocationID\n",
    "\"\"\"\n",
    "comb_df = spark.sql(query)\n",
    "comb_df.createOrReplaceTempView('comb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "576e2169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- dropoff_date: date (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- dropoff_time: integer (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- pickup_zone: string (nullable = true)\n",
      " |-- dropoff_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9a1ec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|pickup_date|dropoff_date|\n",
      "+-----------+------------+\n",
      "| 2020-12-31|  2021-01-01|\n",
      "| 2020-12-31|  2020-12-31|\n",
      "| 2020-12-31|  2021-01-01|\n",
      "| 2020-12-31|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2020-12-31|  2021-01-01|\n",
      "| 2020-12-31|  2020-12-31|\n",
      "| 2020-12-31|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "| 2021-01-01|  2021-01-01|\n",
      "+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT pickup_date, dropoff_date from comb WHERE pickup_time > 0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53e65827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|pickup_date|dropoff_date|\n",
      "+-----------+------------+\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2008-12-31|  2008-12-31|\n",
      "| 2008-12-31|  2008-12-31|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "| 2008-12-31|  2008-12-31|\n",
      "| 2009-01-01|  2009-01-01|\n",
      "+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT pickup_date, dropoff_date from comb WHERE pickup_date < '2020-12-31'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5ece1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:==============================>                          (9 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      total_amount|\n",
      "+-------+------------------+\n",
      "|  count|          15000936|\n",
      "|   mean|  18.7554005111238|\n",
      "| stddev|145.74310219086414|\n",
      "|    min|            -647.8|\n",
      "|    max|          398469.2|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "comb_df.select('total_amount').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24f31dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=======================>                                (7 + 10) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|    trip_distance|\n",
      "+-------+-----------------+\n",
      "|  count|         15000936|\n",
      "|   mean|6.628556730060026|\n",
      "| stddev|671.7240645480986|\n",
      "|    min|              0.0|\n",
      "|    max|        332541.19|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:==============================>                          (9 + 8) / 17]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "comb_df.select('trip_distance').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "277a576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|   passenger_count|\n",
      "+-------+------------------+\n",
      "|  count|          14166908|\n",
      "|   mean|1.4253582362502812|\n",
      "| stddev| 1.044333600318924|\n",
      "|    min|               0.0|\n",
      "|    max|               9.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_df.select('passenger_count').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ee9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
