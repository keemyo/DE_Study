{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aec7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('trip_count_sql').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "757c2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.to_csv(f'file:///{directory}/{filename}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07f0391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 18:16:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , hvfhs_license_num, dispatching_base_num, pickup_datetime, PULocationID, DOLocationID, SR_Flag\n",
      " Schema: _c0, hvfhs_license_num, dispatching_base_num, pickup_datetime, PULocationID, DOLocationID, SR_Flag\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/keemyohaan/Desktop/001.Python/004.%20Study/009.FastCampus/003.DE/01-spark/data/fhvhv_tripdata_2020-03.csv\n",
      "+---+-----------------+--------------------+-------------------+------------+------------+-------+\n",
      "|_c0|hvfhs_license_num|dispatching_base_num|    pickup_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+---+-----------------+--------------------+-------------------+------------+------------+-------+\n",
      "|  0|           HV0005|              B02510|2020-03-01 00:03:40|          81|         159|      N|\n",
      "|  1|           HV0005|              B02510|2020-03-01 00:28:05|         168|         119|      N|\n",
      "|  2|           HV0003|              B02764|2020-03-01 00:03:07|         137|         209|      N|\n",
      "|  3|           HV0003|              B02764|2020-03-01 00:18:42|         209|          80|      Y|\n",
      "|  4|           HV0003|              B02764|2020-03-01 00:44:24|         256|         226|      N|\n",
      "|  5|           HV0003|              B02682|2020-03-01 00:17:23|          79|         263|      N|\n",
      "|  6|           HV0003|              B02764|2020-03-01 00:01:18|          61|          29|      N|\n",
      "|  7|           HV0003|              B02764|2020-03-01 00:43:27|         150|         150|      N|\n",
      "|  8|           HV0003|              B02764|2020-03-01 00:52:23|         150|         210|      N|\n",
      "|  9|           HV0003|              B02764|2020-03-01 00:19:49|          60|         167|      N|\n",
      "| 10|           HV0003|              B02764|2020-03-01 00:29:34|          47|         213|      N|\n",
      "| 11|           HV0003|              B02764|2020-03-01 00:41:44|         213|         235|      N|\n",
      "| 12|           HV0003|              B02765|2020-03-01 00:11:26|         243|         153|      N|\n",
      "| 13|           HV0003|              B02765|2020-03-01 00:28:05|         127|          18|      N|\n",
      "| 14|           HV0003|              B02765|2020-03-01 00:44:28|          18|         169|      N|\n",
      "| 15|           HV0003|              B02765|2020-03-01 00:56:50|          94|         169|      N|\n",
      "| 16|           HV0003|              B02764|2020-03-01 00:56:14|         211|         158|      N|\n",
      "| 17|           HV0003|              B02764|2020-03-01 00:14:15|         246|         107|      N|\n",
      "| 18|           HV0003|              B02764|2020-03-01 00:31:38|         234|           9|      N|\n",
      "| 19|           HV0005|              B02510|2020-03-01 00:26:31|         139|          10|      Y|\n",
      "+---+-----------------+--------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:==========>                                             (3 + 13) / 16]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "directory = '/Users/keemyohaan/Desktop/001.Python/004. Study/009.FastCampus/003.DE/01-spark/data'\n",
    "filename = 'fhvhv_tripdata_2020-03.csv'  # 파일 확장자를 .parquet으로 변경\n",
    "\n",
    "data = spark.read.csv(f'file:///{directory}/{filename}', inferSchema=True, header=True)  # read.parquet 메소드 사용\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04c0f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 핸들링을 위해 temp view\n",
    "data.createOrReplaceTempView('mobility_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5769b5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 18:16:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , hvfhs_license_num, dispatching_base_num, pickup_datetime, PULocationID, DOLocationID, SR_Flag\n",
      " Schema: _c0, hvfhs_license_num, dispatching_base_num, pickup_datetime, PULocationID, DOLocationID, SR_Flag\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/keemyohaan/Desktop/001.Python/004.%20Study/009.FastCampus/003.DE/01-spark/data/fhvhv_tripdata_2020-03.csv\n",
      "+---+-----------------+--------------------+-------------------+------------+------------+-------+\n",
      "|_c0|hvfhs_license_num|dispatching_base_num|    pickup_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+---+-----------------+--------------------+-------------------+------------+------------+-------+\n",
      "|  0|           HV0005|              B02510|2020-03-01 00:03:40|          81|         159|      N|\n",
      "|  1|           HV0005|              B02510|2020-03-01 00:28:05|         168|         119|      N|\n",
      "|  2|           HV0003|              B02764|2020-03-01 00:03:07|         137|         209|      N|\n",
      "|  3|           HV0003|              B02764|2020-03-01 00:18:42|         209|          80|      Y|\n",
      "|  4|           HV0003|              B02764|2020-03-01 00:44:24|         256|         226|      N|\n",
      "+---+-----------------+--------------------+-------------------+------------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from mobility_data limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea8bf35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|pickup_date| trips|\n",
      "+-----------+------+\n",
      "| 2020-03-02|648990|\n",
      "| 2020-03-01|784260|\n",
      "| 2020-03-03|697880|\n",
      "| 2020-03-04|707879|\n",
      "| 2020-03-05|731165|\n",
      "| 2020-03-06|872012|\n",
      "| 2020-03-07|886071|\n",
      "| 2020-03-08|731222|\n",
      "| 2020-03-10|626474|\n",
      "| 2020-03-09|628940|\n",
      "| 2020-03-11|628601|\n",
      "| 2020-03-12|643257|\n",
      "| 2020-03-13|660914|\n",
      "| 2020-03-14|569397|\n",
      "| 2020-03-15|448125|\n",
      "| 2020-03-16|391518|\n",
      "| 2020-03-17|312298|\n",
      "| 2020-03-18|269233|\n",
      "| 2020-03-20|261900|\n",
      "| 2020-03-19|252773|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:================================>                        (9 + 7) / 16]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 22:28:05 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2187033 ms exceeds timeout 120000 ms\n",
      "23/05/07 22:28:05 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT \n",
    "        pickup_date, COUNT(*) as trips\n",
    "    FROM (\n",
    "        SELECT split(pickup_datetime, \" \")[0] as pickup_date\n",
    "        FROM mobility_data\n",
    "    ) aa\n",
    "    GROUP BY pickup_date\n",
    "''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7673022f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
