{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df792f8",
   "metadata": {},
   "source": [
    "# Unstructured vs Semi Structured vs Structured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be8b93",
   "metadata": {},
   "source": [
    "##### 데이터 합치고 추출하기\n",
    "미국의 $2000불 이상의 주식만 가져오기, 가능한 방법은 몇가지일까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751a5e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 22:13:08 WARN Utils: Your hostname, Keemyoui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.35.79 instead (on interface en0)\n",
      "23/04/13 22:13:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 22:13:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/13 22:13:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('category-review-average')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59813dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyValue RDD,\n",
    "    # Key: int\n",
    "    # Values: tuple \n",
    "ticker= sc.parallelize([\n",
    "    (1, (\"Google\", \"GooGL\", \"USA\")),\n",
    "    (2, (\"Netflix\", \"NFLX\", \"USA\")),\n",
    "    (3, (\"Amazon\", \"AMZN\", \"USA\")),\n",
    "    (4, (\"Tesla\", \"TSLA\", \"USA\")),\n",
    "    (5, (\"Samsung\", \"005930\", \"Korea\")),\n",
    "    (6, (\"Kakao\", \"035720\", \"Korea\"))\n",
    "])\n",
    "\n",
    "# KeyValue RDD\n",
    "    # Key: int\n",
    "    # Value: currency\n",
    "prices = sc.parallelize([\n",
    "    (1, (2984, \"USD\")), (2, (645, \"USD\")), \n",
    "    (3, (3518, \"USD\")), (4, (1222, \"USD\")),\n",
    "    (5, (70600, \"KRW\")), (6, (125000, \"KRW\"))    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fa999d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (('Google', 'GooGL', 'USA'), (2984, 'USD'))),\n",
       " (3, (('Amazon', 'AMZN', 'USA'), (3518, 'USD')))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CASE 1: join 먼저, Filter 나중에\n",
    "\n",
    "# key 값(Int)을 통해 Join을 수행함\n",
    "tickerPrice = ticker.join(prices)  \n",
    "tickerPrice.filter(lambda x: x[1][0][2] == \"USA\" and x[1][1][0] > 2000).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55a9ed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (('Google', 'GooGL', 'USA'), (2984, 'USD'))),\n",
       " (3, (('Amazon', 'AMZN', 'USA'), (3518, 'USD')))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CASE 2: filter 먼저 나중에 Join\n",
    "filteredTicker = ticker.filter(lambda x: x[1][2] == 'USA')\n",
    "filteredPrice = prices.filter(lambda x: x[1][0] > 2000)\n",
    "filteredTicker.join(filteredPrice).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa7f27",
   "metadata": {},
   "source": [
    "이 떄 퍼포먼스는 case 2가 더 좋다.  셔플링이 덜되기 때문에"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a82f4",
   "metadata": {},
   "source": [
    "### 구조화된 데이터란\n",
    "\n",
    "##### Unstructured: Free form\n",
    "- 로그 파일\n",
    "- 이미지\n",
    "\n",
    "##### Semi Structured: 헹과 열\n",
    "- csv\n",
    "- json\n",
    "- xml\n",
    "\n",
    "##### Structured: 행과 열 + 데이터 타입 (스키마)\n",
    "- 데이터 베이스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07584ec",
   "metadata": {},
   "source": [
    "### Structured Data vs RDDs\n",
    "\n",
    "##### RDDD에선\n",
    "- 데이터의 구조를 모르기 때문에 데이터를 모르기 때문에 데이터를 다루는 것을 개발자에게 의존\n",
    "- Map, flatMap, filter 등을 통해 유저가 만든 function을 수행\n",
    "\n",
    "##### Structured Data\n",
    "- 데이터의 구조를 이미 알고 있으므로 어떤 테스크를 수행할것인지 정의만 하면 됨\n",
    "- 최적화도 자동으로 할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b13161",
   "metadata": {},
   "source": [
    "### Spark SQL은 구조화된 데이터를 다룰 수 있게 해준다.\n",
    "- 유저가 일일이 function을 정의하는 일 없이 작업을 수행할 수 있다.\n",
    "- 자동으로 연산이 최적화 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d430962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
