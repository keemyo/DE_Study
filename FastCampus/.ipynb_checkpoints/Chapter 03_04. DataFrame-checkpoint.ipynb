{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17afdf41",
   "metadata": {},
   "source": [
    "이제까지\n",
    "- Spark SQL 사용법\n",
    "이번 강의에선\n",
    "- DataFrame의 사용법\n",
    "    - DataFrame의 데이터 타입\n",
    "    - DataFrame에서 가능한 연산들\n",
    "    - DataFrame에서의 Aggregation 작업들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175f2ca",
   "metadata": {},
   "source": [
    "# DataFrame은 관계형 데이터 \n",
    "- 한마디로: 관계형 데이터셋(RDD + Relation)에 적용한 것\n",
    "- RDD가 함수형 API를 가졌다면 DataFrame은 선언형 API\n",
    "- 자동으로 최적화가 가능\n",
    "    - Schema를 통한 최적화\n",
    "- 타입이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eadd861",
   "metadata": {},
   "source": [
    "# DataFrame의 특징\n",
    "- 지연실행(Lazy Execution)\n",
    "- 분산 저장\n",
    "- Immutable\n",
    "    - 생성한 이후에 변경할 수 없음을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e705bf9",
   "metadata": {},
   "source": [
    "## DataFrame: RDD의 확장판\n",
    "- 지연 실행(Lazy Execution)\n",
    "- 분산 저장\n",
    "- Immutable\n",
    "- 열(Row) 객체가 있다.\n",
    "- SQL 쿼리를 실행할 수 있다.\n",
    "- 스키마를 가질 수 있고 이를 통해 성능을 더욱 최적화 할 수 있다.\n",
    "- CSV, JSON, Hive 등으로 읽어오거나 변환이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2765c",
   "metadata": {},
   "source": [
    "# DataFrame의 스키마를 확인하는 법\n",
    "- Dtypes\n",
    "- show()\n",
    "    - 테이블 형태로 데이터를 출력\n",
    "    - 첫 20개의 열만 보여준다.\n",
    "- PrintSchema()\n",
    "    - 스키마를 트리 형태로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797f048",
   "metadata": {},
   "source": [
    "# 복잡한 DataType들\n",
    "- ArrayType\n",
    "- MapType\n",
    "- StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506cd3d2",
   "metadata": {},
   "source": [
    "# DataFrame Operations\n",
    "SQL과 비슷한 작업이 가능하다\n",
    "- Select\n",
    "- Where \n",
    "- Limit\n",
    "- OrderBy\n",
    "- GROUPBy\n",
    "- Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bdeff",
   "metadata": {},
   "source": [
    "### 1) Select \n",
    "사용자가 원하는 Column이나 데이터를 추출하는데 사용\n",
    "\n",
    "- df.select('*').collect()\n",
    "- df.select('name', 'age').collect()\n",
    "- df.select(df.name, (df.age +1).alias('age')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720b640",
   "metadata": {},
   "source": [
    "### 2) Agg\n",
    "Aggregate의 약자로, 그룹핑 후 데이터를 하나로 합치는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01e5b7",
   "metadata": {},
   "source": [
    "- df.agg({'age': 'max'}).collect()\n",
    "    - age 컬럼 안에서 Max값 구하기\n",
    "- from pyspark.sql import functions as F\n",
    "    - df.agg(F.min(df.age)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce9474",
   "metadata": {},
   "source": [
    "### 3) GroupBy\n",
    "사용자가 지정한 Column을 기준으로 데이터를 Grouping하는 작업\n",
    "\n",
    "- df.groupBy().avg().collect()\n",
    "- sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
    "- sorted(df.groupBy(df.name).avg().collect())\n",
    "- sorted(df.groupBy(['name' df.age]).count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae102c60",
   "metadata": {},
   "source": [
    "### 4) Join\n",
    "다른 DataFrame과 사용자가 지정한 Column을 기준으로 합치는 작업\n",
    "\n",
    "- df.join(df2, 'name').select(df.name, df2.height).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644e77c",
   "metadata": {},
   "source": [
    "# DataFrame 조작하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84812d0",
   "metadata": {},
   "source": [
    "- sparkSession.builder: \"SparkSession\" 클래스의 빌더를 호출해서 SparkSession을 생성하기 위핸 빌더 객체를 가져옵니다.\n",
    "- Master('local'): SparkSession의 마스터 URL을 설정합니다. 여기서 'Local'은 로컬 모드로 실행하겠다는 것을 의미합니다. 이는 스파크 단일 노드에서 실행하고 모든 스파크 작업을 로컬 머신에서 처리하겠다는 것입니다.\n",
    "- appName('create-dataframe'): SparkSession의 애플리케이션 이름을 설정합니다. 이 이름은 Spark UI에서 애플리케이션을 구분하는 데 사용됩니다. 여기서 애플리케이션 이름은 'create-dataframe'입니다.\n",
    "- getOrCreate(): 설정된 빌더 객체를 사용하여 SparkSession을 생성합니다. 이 메소드는 SparkSession이 이미 존재하는 경우 해당 세션을 반환하고, 없는 경우 새로운 세션을 생성하여 반환합니다. 이렇게 하면 여러 번 코드를 실행할 때 새로운 SparkSession을 생성하는 대신 기존 세션을 재사용할 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7240fb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 16:37:57 WARN Utils: Your hostname, Keemyoui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.35.79 instead (on interface en0)\n",
      "23/05/07 16:37:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/07 16:37:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/07 16:37:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('create-dataframe').getOrCreate()\n",
    "\n",
    "stocks = [\n",
    "    ('Google', 'GOOGL', 'USA', 2984, 'USD'),\n",
    "    ('Netflix', 'NFLX', 'USA', 645, 'USD'),   \n",
    "    ('Amazon', 'AMZN', 'USA', 3518, 'USD'),   \n",
    "    ('Tesla', 'TSLA', 'USA', 1222, 'USD'),   \n",
    "    ('Samsung', '005930', 'Korea', 70600, 'KRW'),  \n",
    "    ('Kakao', '035720', 'Korea', 125000, 'KRW')]     \n",
    "\n",
    "schema = ['name', 'ticker', 'country', 'price', 'currency']\n",
    "df = spark.createDataFrame(data = stocks, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3712d4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+------+--------+\n",
      "|   name|ticker|country| price|currency|\n",
      "+-------+------+-------+------+--------+\n",
      "| Google| GOOGL|    USA|  2984|     USD|\n",
      "|Netflix|  NFLX|    USA|   645|     USD|\n",
      "| Amazon|  AMZN|    USA|  3518|     USD|\n",
      "|  Tesla|  TSLA|    USA|  1222|     USD|\n",
      "|Samsung|005930|  Korea| 70600|     KRW|\n",
      "|  Kakao|035720|  Korea|125000|     KRW|\n",
      "+-------+------+-------+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32bbb231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|   name|country|price|\n",
      "+-------+-------+-----+\n",
      "|Netflix|    USA|  645|\n",
      "|  Tesla|    USA| 1222|\n",
      "| Google|    USA| 2984|\n",
      "| Amazon|    USA| 3518|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# order by\n",
    "usaStockDF = df.select('name', 'country', 'price').where('country == \"USA\"').orderBy('price')\n",
    "usaStockDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b919b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|currency|max(price)|\n",
      "+--------+----------+\n",
      "|     KRW|    125000|\n",
      "|     USD|      3518|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby\n",
    "df.groupBy('currency').max('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95c1bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|currency|avg(price)|\n",
      "+--------+----------+\n",
      "|     KRW|   97800.0|\n",
      "|     USD|   2092.25|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function 사용하기\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "df.groupBy('currency').agg(avg('price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ce8521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|currency|count(price)|\n",
      "+--------+------------+\n",
      "|     KRW|           2|\n",
      "|     USD|           4|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('currency').agg(count('price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec7086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
