{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a795d6",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31d5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('logistic-regression').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241400ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e818b7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|  1.0| [0.0,1.1,0.1]|\n",
      "|  0.0|[2.0,1.0,-1.0]|\n",
      "|  0.0| [2.0,1.3,1.0]|\n",
      "|  1.0|[0.0,1.2,-0.5]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], ['label', 'features'])\n",
    "\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "447db31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 리그레션의 인스턴스를 만들어보자\n",
    "lr = LogisticRegression(maxIter=30, regParam=0.01)\n",
    "mode = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddcde99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|  1.0|[-1.0,1.5,1.3]|\n",
      "|  0.0|[3.0,2.0,-0.1]|\n",
      "|  1.0|[0.0,2.2,-1.5]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),    \n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], ['label', 'features'])\n",
    "\n",
    "\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cb50f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0|[-1.0,1.5,1.3]|[-6.2435550918400...|[0.00193916823498...|       1.0|\n",
      "|  0.0|[3.0,2.0,-0.1]|[5.45228608726759...|[0.99573180142693...|       0.0|\n",
      "|  1.0|[0.0,2.2,-1.5]|[-4.4104172202339...|[0.01200425500655...|       1.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41750a",
   "metadata": {},
   "source": [
    "# PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa17c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/11 20:16:39 WARN Utils: Your hostname, Keemyoui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.35.79 instead (on interface en0)\n",
      "23/06/11 20:16:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/11 20:16:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/11 20:16:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local').appName('logistic-regression').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac7bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd0f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark가 있을 때 1이 들어오는 간단한 로직\n",
    "training = spark.createDataFrame([\n",
    "    (0, 'a b c d e spark', 1.0),\n",
    "    (1, 'b d', 0.0),\n",
    "    (2, 'spark f g h', 1.0),\n",
    "    (3, 'hadoop maprecedure', 0.0)\n",
    "], ['id', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36358d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자들을 split, input으로 text가 들어갈거임\n",
    "tokenizer = Tokenizer(inputCol = 'text', outputCol='words')\n",
    "# TermFrequency도 인풋 = 토크나이저의 getOutputCol : 토크나이저가 쓰는 outputcol을 쓴다.\n",
    "hashingTF = HashingTF(inputCol = tokenizer.getOutputCol(), outputCol='features')\n",
    "lr = LogisticRegression(maxIter=30, regParam=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bfb490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline을 통해 model을 나온다. (model이 아웃풋)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e52f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (4, 'spark i j k'),    \n",
    "    (5, 'l m n'),    \n",
    "    (6, 'spark hadoop spark'),    \n",
    "    (7, 'apache hadoop'),        \n",
    "], ['id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10600c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+----------+\n",
      "| id|              text|         probability|prediction|\n",
      "+---+------------------+--------------------+----------+\n",
      "|  4|       spark i j k|[0.63102699631690...|       0.0|\n",
      "|  5|             l m n|[0.98489377609773...|       0.0|\n",
      "|  6|spark hadoop spark|[0.13563147748816...|       1.0|\n",
      "|  7|     apache hadoop|[0.99563405823116...|       0.0|\n",
      "+---+------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "prediction.select(['id', 'text', 'probability', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06882f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
